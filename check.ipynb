{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bisect import bisect_right\n",
    "import torch\n",
    "\n",
    "from configurations import RMV_NON_UNIQUE_TIMESTAMP, NON_UNIQUE_TIMESTAMP_INDEX_RANGE, TRAIN_DATA_BORDER, TEST_DATA_BORDER\n",
    "import macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cache/preprocessed/df000.pickle', 'rb') as fp:\n",
    "    train_df, test_df, full_sen_list, df_status = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_sen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configurations import SEQ_LEN\n",
    "full_sensor_list = full_sen_list\n",
    "def extract_features(df, error_sensor_timestamps_dict):\n",
    "    sen_num = len(full_sensor_list)\n",
    "    dfs = []\n",
    "    for sen in full_sensor_list:\n",
    "        diff_df = df[df.sensor==sen].diff() # on~off and off~on time\n",
    "        diff_df = diff_df[diff_df.event == -1] # on~off time only\n",
    "        diff_ser = diff_df.timestamp.reset_index(drop=True)\n",
    "        diff_flt = diff_ser.apply(datetime.timedelta.total_seconds)\n",
    "        true_df = df[(df.sensor==sen) & (df.event)].copy().reset_index(drop=True)\n",
    "        true_df['timediff'] = diff_flt\n",
    "        true_df.set_index('timestamp', inplace=True)\n",
    "        base_np = np.zeros((true_df.shape[0], sen_num+1))\n",
    "        feature_df = pd.DataFrame(base_np,\n",
    "                columns=[*full_sensor_list, 'timediff'],\n",
    "                index=true_df.index\n",
    "            )\n",
    "        feature_df.loc[:, sen] = true_df.timediff\n",
    "        dfs.append(feature_df)\n",
    "    concated = pd.concat(dfs, axis=0).sort_index()\n",
    "    concated = concated[~concated.isna().any(axis=1)]\n",
    "    concated['timediff'] = concated.index.to_series().diff().iloc[1:].apply(datetime.timedelta.total_seconds)\n",
    "    concated = concated.iloc[1:]\n",
    "    idxs = concated.index\n",
    "    base_np = np.zeros((concated.shape[0], len(error_sensor_timestamps_dict)))\n",
    "    error_counts_df = pd.DataFrame(base_np,\n",
    "            columns=error_sensor_timestamps_dict.keys(),\n",
    "            index=idxs\n",
    "        )\n",
    "    for error_sensor, times in error_sensor_timestamps_dict.items():\n",
    "        for timestamp in times:\n",
    "           idx = bisect_right(idxs, timestamp)\n",
    "           error_counts_df.iloc[idx].loc[error_sensor] += 1\n",
    "    error_cum = error_counts_df.cumsum(axis=0)\n",
    "    error_cum = error_cum.iloc[SEQ_LEN-1:].reset_index(drop=True) - \\\n",
    "        error_cum[:-100].shift(1,fill_value=0).reset_index(drop=True)\n",
    "    error_cum = error_cum[:-1]\n",
    "    error_cum.set_index(error_counts_df.index[:-SEQ_LEN], drop=True, inplace=True)\n",
    "    return concated, error_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.pickle', 'rb') as fp:\n",
    "    X = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import generate_scaler\n",
    "scaler = generate_scaler('MinMax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. MinMaxScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scaler\u001b[39m.\u001b[39;49mfit_transform(X)\n",
      "File \u001b[0;32m~/.conda/envs/nlp-pr/lib/python3.9/site-packages/sklearn/base.py:699\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    698\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    700\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/.conda/envs/nlp-pr/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:363\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y)\n",
      "File \u001b[0;32m~/.conda/envs/nlp-pr/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:396\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    395\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 396\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, reset\u001b[39m=\u001b[39;49mfirst_pass,\n\u001b[1;32m    397\u001b[0m                         estimator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    398\u001b[0m                         force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    400\u001b[0m data_min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmin(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    401\u001b[0m data_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmax(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/nlp-pr/lib/python3.9/site-packages/sklearn/base.py:421\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    420\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(y, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m y \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 421\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    422\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    423\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/nlp-pr/lib/python3.9/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[1;32m     67\u001b[0m             \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[1;32m     68\u001b[0m                                  args[\u001b[39m-\u001b[39mextra_args:])]\n",
      "File \u001b[0;32m~/.conda/envs/nlp-pr/lib/python3.9/site-packages/sklearn/utils/validation.py:716\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    713\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to convert array of bytes/strings \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    714\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minto decimal numbers with dtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m--> 716\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m                      \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name))\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    720\u001b[0m     _assert_all_finite(array,\n\u001b[1;32m    721\u001b[0m                        allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. MinMaxScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.pickle', 'rb') as fp:\n",
    "    label_scores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, preds= zip(*label_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.sigmoid(torch.tensor(preds))\n",
    "scores = scores.data.numpy()\n",
    "labels = np.array(labels)\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994.0675"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(scores - labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6774543124999999"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "idx = 2\n",
    "auc = roc_auc_score(labels[:,idx], scores[:,idx])\n",
    "auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('nlp-pr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65a5769488afb7ff87115181d75743536f5ef5f112e45af78c5f6b9d71e345e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
